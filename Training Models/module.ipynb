{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Training:\n",
    "    def pretrained_network(name='inceptionv3',img_height=300,img_width=300):\n",
    "        \"\"\"\n",
    "        Insert the name of the pretrained model in small letters without spacing, the function will return the preprocessing layer and the model layer pre-trained on imagenet.\n",
    "        \"\"\"\n",
    "        if name == 'inceptionv3':\n",
    "            preprocessing = tf.keras.applications.inception_v3.preprocess_input\n",
    "            pre_trained = tf.keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))\n",
    "        elif name == 'resnet50':\n",
    "            preprocessing = tf.keras.applications.resnet50.preprocess_input\n",
    "            pre_trained = tf.keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))\n",
    "        elif name == 'vgg16':\n",
    "            preprocessing = tf.keras.applications.vgg16.preprocess_input\n",
    "            pre_trained = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))\n",
    "        elif name == 'efficientnetv2m':\n",
    "            preprocessing = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "            pre_trained = tf.keras.applications.efficientnet_v2.EfficientNetV2M(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))\n",
    "        elif name == 'efficientnetb3':\n",
    "            preprocessing = tf.keras.applications.efficientnet.preprocess_input\n",
    "            pre_trained = tf.keras.applications.efficientnet.EfficientNetB3(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))\n",
    "        else:\n",
    "            print('Unknown model has been inserted, please insert the model name in small letters without spacing. The pre-trained model has been defaulted to InceptionV3 in this model.')   \n",
    "            preprocessing = tf.keras.applications.inception_v3.preprocess_input\n",
    "            pre_trained = tf.keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height,img_width,3))   \n",
    "        return preprocessing, pre_trained\n",
    "    \n",
    "    def training(model,train_dataset,validation_dataset,total_epochs,save_weights,patience,Earlystop,train_log,callbacks,optimiser):\n",
    "        best_weights = None\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_acc=float(0)\n",
    "        all_history = {}\n",
    "        all_history['train_loss'] = []\n",
    "        all_history['train_acc'] = []\n",
    "        all_history['val_loss'] = []\n",
    "        all_history['val_acc'] = []\n",
    "        min_lr = 1e-5\n",
    "        if Earlystop == False:\n",
    "            Earlystop = total_epochs\n",
    "        if patience == False:\n",
    "            patience = total_epochs\n",
    "        earlystop_counter = 0\n",
    "        with tf.device('/GPU:0'):\n",
    "            for i in range(total_epochs):\n",
    "                # print(\"Epoch: {}\".format(i))\n",
    "                history = model.fit(train_dataset\n",
    "                                    ,epochs=i+1\n",
    "                                    ,initial_epoch=i\n",
    "                                    ,validation_data=validation_dataset\n",
    "                                    ,callbacks = [callbacks]\n",
    "                                    # ,verbose=0\n",
    "                                    )\n",
    "                val_loss = history.history['val_loss'][-1]\n",
    "                val_categorical_accuracy = history.history['val_categorical_accuracy'][-1]\n",
    "                loss = history.history['loss'][-1]\n",
    "                categorical_accuracy = history.history['categorical_accuracy'][-1]\n",
    "                all_history['train_loss'].append(loss)\n",
    "                all_history['train_acc'].append(categorical_accuracy)\n",
    "                all_history['val_loss'].append(val_loss)\n",
    "                all_history['val_acc'].append(val_categorical_accuracy)\n",
    "                if train_log:\n",
    "                    log = open(train_log, 'a')\n",
    "                    writer = csv.writer(log)\n",
    "                    new_dict = [i+1,history.history['loss'][-1],history.history['categorical_accuracy'][-1],val_loss,history.history['val_categorical_accuracy'][-1]]\n",
    "                    writer.writerow(new_dict)\n",
    "                    log.close()\n",
    "                # val_improve = val_loss<best_val_loss and\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_weights = model.get_weights()\n",
    "                    model.save_weights(save_weights)\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    earlystop_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    earlystop_counter +=1\n",
    "\n",
    "                # Check if we have reached the patience limit\n",
    "                if patience_counter == patience:\n",
    "                    # Load the best weights back into the model_fine\n",
    "                    model.set_weights(best_weights)\n",
    "                    # Reduce the learning rate\n",
    "                    if optimiser.lr > min_lr:\n",
    "                        optimiser.lr = optimiser.lr * 0.1\n",
    "                    # Reset the patience counter\n",
    "                    patience_counter = 0\n",
    "                if earlystop_counter == Earlystop:\n",
    "                    print(f'Early Stop at Epoch: {i+1}')\n",
    "                    break\n",
    "        return model, all_history\n",
    "\n",
    "    def build_model(pretrained_model,trainable_layers=False,augmentation=False,img_height=300,img_width=300):\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "          tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
    "          tf.keras.layers.RandomRotation((0,0.2),fill_mode='reflect'),\n",
    "          tf.keras.layers.RandomZoom(height_factor=(-0.2,0.2),width_factor=(-0.2,0.2),fill_mode='reflect'),\n",
    "          tf.keras.layers.RandomTranslation(height_factor=(-0.1,0.1),width_factor=(-0.1,0.1),fill_mode='reflect')\n",
    "          ])\n",
    "        preprocessing,pre_trained = Model_Training.pretrained_network(name=pretrained_model,img_height=img_height,img_width=img_width)\n",
    "        if trainable_layers:\n",
    "            pre_trained.trainable = True\n",
    "            for layer in pre_trained.layers:\n",
    "              if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                  layer.trainable = False\n",
    "            for layer in pre_trained.layers[:-trainable_layers]:\n",
    "              layer.trainable = False\n",
    "        else:\n",
    "            pre_trained.trainable = False\n",
    "        tfinput = tf.keras.layers.Input(shape=(img_height,img_width,3))\n",
    "        if augmentation:\n",
    "            data_augment = data_augmentation(tfinput)\n",
    "            pre_process = preprocessing(data_augment)\n",
    "        else:\n",
    "            pre_process = preprocessing(tfinput)\n",
    "        tl_model=pre_trained(pre_process,training=False)\n",
    "        flatten = tf.keras.layers.Flatten()(tl_model)\n",
    "        x = tf.keras.layers.Dense(8,activation = 'relu')(flatten)\n",
    "        x = tf.keras.layers.Dense(8,activation='relu')(x)\n",
    "        x= tf.keras.layers.Dropout(0.5)(x)\n",
    "        x = tf.keras.layers.Dense(8,activation='relu')(x)\n",
    "        output = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "        model = tf.keras.models.Model(tfinput,output)\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def main(           pretrained_model,\n",
    "                        train_dataset,\n",
    "                        validation_dataset,\n",
    "                        epochs,\n",
    "                        patience=False,\n",
    "                        Earlystop=False,\n",
    "                        augmentation = True,\n",
    "                        trainable_layers = False,\n",
    "                        train_log=False,\n",
    "                        load_weights = False,\n",
    "                        save_weights = '/home/jj/FYP/Checkpoint/Placeholder/bestmodel',\n",
    "                        learning_rate=1e-3,\n",
    "                        optimiser=tf.keras.optimizers.Adam(),\n",
    "                        losses = tf.keras.losses.CategoricalCrossentropy(),\n",
    "                        metrics = [tf.keras.metrics.CategoricalAccuracy(),tf.keras.metrics.Precision(class_id=0),tf.keras.metrics.Precision(class_id=1)],\n",
    "                        callbacks = [],\n",
    "                        img_height=300,img_width=300):\n",
    "        model = Model_Training.build_model(pretrained_model,trainable_layers,augmentation,img_height,img_width)\n",
    "        optimiser.lr = learning_rate\n",
    "        model.compile(\n",
    "            optimizer= optimiser,\n",
    "            loss=losses,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        if train_log:\n",
    "            new_dict = {'Epoch','train_loss','train_accuracy','val_loss','val_accuracy'}\n",
    "            log = open(train_log, 'a')\n",
    "            writer = csv.writer(log)\n",
    "            writer.writerow(new_dict)\n",
    "            log.close()\n",
    "        if load_weights:\n",
    "            model.load_weights(load_weights)\n",
    "        model, history = Model_Training.training(model,train_dataset,validation_dataset,epochs,save_weights,patience,Earlystop,train_log,callbacks,optimiser)\n",
    "\n",
    "\n",
    "        return model, history\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c1e3757070209b65134bdc00912b0c0bbe6b0fe0ffdf5f1586851a346c26673"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
